{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build format VAE network\n",
    "Test VAE build before including it in larger training framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "Install tensorflow:\n",
    "``%pip install tensorflow``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "tf.random.set_seed(2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create sampling layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_encoder():    \n",
    "#     #filter_1 = 3 #32\n",
    "#     #filter_2 = 2 #64\n",
    "#     #kernel_size = 5 #3\n",
    "#     dense_size = 16; \n",
    "#     encoder_inputs = keras.Input(shape=(20, 20,3)) # enter cut-out shape (20,20,3)\n",
    "#     x = layers.Conv2D(filter_1, kernel_size, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n",
    "#     x = layers.Conv2D(filter_2, kernel_size, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "#     x = layers.Flatten()(x) # to vector\n",
    "#     x = layers.Dense(dense_size, activation=\"relu\")(x) # linked layer\n",
    "#     z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "#     z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "#     z = Sampling()([z_mean, z_log_var])\n",
    "#     encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "#     encoder.summary()\n",
    "#     return encoder_inputs, encoder, z , z_mean, z_log_var\n",
    "\n",
    "\n",
    "def make_encoder(cutout_size,n_bands,\n",
    "                 filter_1,filter_2,\n",
    "                 kernel_size_1,kernel_size_2,\n",
    "                 dense_size,latent_dim):    \n",
    "    encoder_inputs = keras.Input(shape=(cutout_size, cutout_size,n_bands)) # enter cut-out shape (20,20,3)\n",
    "    x = layers.Conv2D(filter_1, kernel_size_1, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n",
    "    x = layers.Conv2D(filter_2, kernel_size_2, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "    x = layers.Conv2D(filter_2, kernel_size_2, activation=\"relu\", strides=1, padding=\"same\")(x)\n",
    "    x = layers.Flatten()(x) # to vector\n",
    "    x = layers.Dense(dense_size, activation=\"relu\")(x) # linked layer\n",
    "    z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "    z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "    z = Sampling()([z_mean, z_log_var])\n",
    "    encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "    encoder.summary()\n",
    "    \n",
    "    return encoder_inputs, encoder, z , z_mean, z_log_var\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_decoder(): \n",
    "#     latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "#     x = layers.Dense(5 * 5 * filter_2, activation=\"relu\")(latent_inputs) # -- shape corresponding to encoder\n",
    "#     x = layers.Reshape((5, 5, filter_2))(x)\n",
    "#     x = layers.Conv2DTranspose(filter_2, kernel_size, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "#     x = layers.Conv2DTranspose(filter_1, kernel_size, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "#     decoder_outputs = layers.Conv2DTranspose(3, 3, activation=\"sigmoid\", padding=\"same\")(x) # (1,3) or (3,3)\n",
    "#     decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "#     decoder.summary()\n",
    "#     return decoder\n",
    "\n",
    "def make_decoder(latent_dim,encoder,\n",
    "                 filter_1,filter_2,\n",
    "                 kernel_size_1,kernel_size_2,\n",
    "                 n_bands): \n",
    "    latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "    # get shape of last layer in encoder before flattning\n",
    "    flat_layer = [layer for layer in encoder.layers if 'flatten' in layer.name] \n",
    "    flat_input = flat_layer[-1].input_shape # input shape of flat layer to be used to reconstruct; (None, 5,5,16) or smth\n",
    "    # x = layers.Dense(5 * 5 * filter_2, activation=\"relu\")(latent_inputs) # -- shape corresponding to encoder\n",
    "    # x = layers.Reshape((5, 5, filter_2))(x)\n",
    "    x = layers.Dense(flat_input[1] * flat_input[2] * filter_2, activation=\"relu\")(latent_inputs) # -- shape corresponding to encoder\n",
    "    x = layers.Reshape((flat_input[1], flat_input[2], filter_2))(x)\n",
    "    x = layers.Conv2DTranspose(filter_2, kernel_size_2, activation=\"relu\", strides=1, padding=\"same\")(x)\n",
    "    x = layers.Conv2DTranspose(filter_2, kernel_size_2, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "    x = layers.Conv2DTranspose(filter_1, kernel_size_1, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "    # decoder_outputs = layers.Conv2DTranspose(3, 3, activation=\"sigmoid\", padding=\"same\")(x) # (1,3) or (3,3)\n",
    "    decoder_outputs = layers.Conv2DTranspose(n_bands, n_bands, activation=\"sigmoid\", padding=\"same\")(x) # (1,3) or (3,3)\n",
    "    decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "    decoder.summary()\n",
    "    return decoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define VAE as model\n",
    "With custom train_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update: instead of defining VAE as class, use function-wise definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define VAE model.\n",
    "def make_vae(encoder_inputs, z, z_mean, z_log_var, decoder,alpha=5):\n",
    "    outputs = decoder(z)\n",
    "    vae = tf.keras.Model(inputs=encoder_inputs, outputs=outputs, name=\"vae\")\n",
    "\n",
    "    # Add KL divergence regularization loss.\n",
    "    reconstruction = decoder(z)\n",
    "    reconstruction_loss = tf.reduce_mean(\n",
    "        tf.reduce_sum(\n",
    "            keras.losses.binary_crossentropy(encoder_inputs, reconstruction), axis=(1, 2)\n",
    "                )\n",
    "            )\n",
    "    kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "    kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "\n",
    "    # Play witht different alpha: -2, 0 , 1 ,2 ; 0.2 ; -0.5 ; 50\n",
    "    total_loss = reconstruction_loss +  alpha * kl_loss # alpha is custom\n",
    "    # vae.add_loss(total_loss)\n",
    "    \n",
    "    vae.add_loss(reconstruction_loss)\n",
    "    vae.add_loss(kl_loss)\n",
    "\n",
    "    vae.add_metric(kl_loss, name='kl_loss', aggregation='mean')\n",
    "    vae.add_metric(reconstruction_loss, name='mse_loss', aggregation='mean')\n",
    "\n",
    "\n",
    "    return vae\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_21 (InputLayer)          [(None, 100, 100, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d_30 (Conv2D)             (None, 50, 50, 32)   2432        ['input_21[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_31 (Conv2D)             (None, 25, 25, 16)   12816       ['conv2d_30[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_32 (Conv2D)             (None, 25, 25, 16)   6416        ['conv2d_31[0][0]']              \n",
      "                                                                                                  \n",
      " flatten_10 (Flatten)           (None, 10000)        0           ['conv2d_32[0][0]']              \n",
      "                                                                                                  \n",
      " dense_20 (Dense)               (None, 100)          1000100     ['flatten_10[0][0]']             \n",
      "                                                                                                  \n",
      " z_mean (Dense)                 (None, 4)            404         ['dense_20[0][0]']               \n",
      "                                                                                                  \n",
      " z_log_var (Dense)              (None, 4)            404         ['dense_20[0][0]']               \n",
      "                                                                                                  \n",
      " sampling_10 (Sampling)         (None, 4)            0           ['z_mean[0][0]',                 \n",
      "                                                                  'z_log_var[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,022,572\n",
      "Trainable params: 1,022,572\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "input_21\n",
      "[(None, 100, 100, 3)]\n",
      "conv2d_30\n",
      "(None, 100, 100, 3)\n",
      "conv2d_31\n",
      "(None, 50, 50, 32)\n",
      "conv2d_32\n",
      "(None, 25, 25, 16)\n",
      "flatten_10\n",
      "(None, 25, 25, 16)\n",
      "dense_20\n",
      "(None, 10000)\n",
      "z_mean\n",
      "(None, 100)\n",
      "z_log_var\n",
      "(None, 100)\n",
      "sampling_10\n",
      "[(None, 4), (None, 4)]\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_22 (InputLayer)       [(None, 4)]               0         \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 10000)             50000     \n",
      "                                                                 \n",
      " reshape_10 (Reshape)        (None, 25, 25, 16)        0         \n",
      "                                                                 \n",
      " conv2d_transpose_40 (Conv2D  (None, 25, 25, 16)       6416      \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " conv2d_transpose_41 (Conv2D  (None, 50, 50, 16)       6416      \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " conv2d_transpose_42 (Conv2D  (None, 100, 100, 32)     12832     \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " conv2d_transpose_43 (Conv2D  (None, 100, 100, 3)      867       \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 76,531\n",
      "Trainable params: 76,531\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "filter1 = 32 \n",
    "filter2 = 16\n",
    "kernelSize1 = 5\n",
    "kernelSize2=kernelSize1\n",
    "denseSize = 100 # 16 \n",
    "\n",
    "sizeCutOut=100\n",
    "bands = 3, 2, 1  # S2\n",
    "# bands=[1] # S1\n",
    "\n",
    "latentDim = 4\n",
    "\n",
    "encoder_inputs, encoder, z, z_mean, z_log_var = make_encoder(\n",
    "                            sizeCutOut,len(bands),\n",
    "                            filter1,filter2,\n",
    "                            kernelSize1,kernelSize2,\n",
    "                            denseSize,latentDim)\n",
    "\n",
    "# analyse encoder layers\n",
    "for layer in encoder.layers:\n",
    "    print(layer.name)\n",
    "    print(layer.input_shape)\n",
    "    \n",
    "# layer_flatten_shape = encode\n",
    "# print(encoder.layers)\n",
    "flat_layer = [layer for layer in encoder.layers if 'flatten' in layer.name] # want flat_layer input_shape to usee in decoder\n",
    "dense_layer = [layer for layer in encoder.layers if 'dense' in layer.name]\n",
    "# print(flat_layer[-1].input_shape,dense_layer[-1].input_shape)\n",
    "\n",
    "\n",
    "\n",
    "# print(encoder_inputs)\n",
    "decoder = make_decoder(latentDim,encoder,\n",
    "                       filter1,filter2,\n",
    "                       kernelSize1,kernelSize2,\n",
    "                       n_bands=len(bands))\n",
    "vae = make_vae(encoder_inputs, z, z_mean, z_log_var, decoder)\n",
    "\n",
    "vae.compile(optimizer=keras.optimizers.Adam(learning_rate = 0.001))\n",
    "\n",
    "# vae.metrics_tensors.append(kl_loss)\n",
    "# vae.metrics_names.append(\"kl_loss\")\n",
    "\n",
    "# vae.metrics_tensors.append(reconstruction_loss)\n",
    "# vae.metrics_names.append(\"mse_loss\")\n",
    "\n",
    "\n",
    "# vae.save('./test_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 0 works:\n",
    "# filter1 = 64 \n",
    "# filter2 = 16 \n",
    "# kernelSize = 5\n",
    "# denseSize = 16\n",
    "\n",
    "\n",
    "# # this did not work (out of memory)\n",
    "# filter1 = 128 \n",
    "# filter2 = 32 \n",
    "# kernelSize1 = 8\n",
    "# kernelSize2= 5\n",
    "# denseSize = 16\n",
    "\n",
    "filter1 = 64 \n",
    "filter2 = 32 \n",
    "kernelSize1 = 5\n",
    "kernelSize2= 5\n",
    "denseSize = 16\n",
    "\n",
    "sizeCutOut=20\n",
    "bands = 3, 2, 1 \n",
    "\n",
    "latentDim = 4\n",
    "\n",
    "encoder_inputs, encoder, z, z_mean, z_log_var = make_encoder(\n",
    "                            sizeCutOut,len(bands),\n",
    "                            filter1,filter2,\n",
    "                            kernelSize1,kernelSize2,\n",
    "                            denseSize,latentDim)\n",
    "\n",
    "decoder = make_decoder(latentDim,filter1,filter2,kernelSize1,kernelSize2)\n",
    "vae = make_vae(encoder_inputs, z, z_mean, z_log_var, decoder)\n",
    "vae.compile(optimizer=keras.optimizers.Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# number of training steps in 1 epoch \n",
    "model.fit( steps_per_epoch=None):  When training with input tensors such as TensorFlow data tensors, the default None is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined\n",
    "\n",
    "So I have  204,259 samples (window size =100 )\n",
    "batch_size = 32 (default; do not set it yourself when using tf.datasets\n",
    "Number of steps: 2766"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "204259 / 32\n",
    "204259 / 2766\n",
    "# 204259 / 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test read config file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import configparser\n",
    "\n",
    "def parse_config(config):\n",
    "    \"\"\" Parse input arguments from dictionary or config file \"\"\"\n",
    "    if not isinstance(config, dict):\n",
    "        parser = configparser.ConfigParser()\n",
    "        parser.read(config)\n",
    "        config = parser[\"train-VAE\"]\n",
    "\n",
    "    catPath = config['catalogPath']\n",
    "    labPath = config['labelsPath']\n",
    "    outputDir = config['outputDirectory']\n",
    "    sizeTestSet = int(config['sizeTestSet'])\n",
    "    sizeValSet = int(config['sizeValidationSet'])\n",
    "    roiFile = config['ROIFile']\n",
    "    #\n",
    "    bands = [int(i) for i in config['bands'].split(\" \")]\n",
    "    bands_names = [int(i) for i in config['bands'].split(\" \")]\n",
    "    sizeCutOut = int(config['sizeCutOut'])\n",
    "    sizeStep = int(config['sizeStep'])\n",
    "    stride = int(config['stride'])\n",
    "    #DATA\n",
    "    # balanceRatio = float(config['balanceRatio'])\n",
    "    file_DMGinfo = config['tiledDamagePixelsCountFile']\n",
    "    # normThreshold = float(config['normalizationThreshold'])\n",
    "    normThreshold = [float(i) for i in config['normalizationThreshold'].split(\" \")]\n",
    "    # MODEL\n",
    "    filter1 = int(config['filter1'])\n",
    "    filter2 = int(config['filter2'])\n",
    "    kernelSize1 = int(config['kernelSize1'])\n",
    "    kernelSize2 = int(config['kernelSize2'])\n",
    "    denseSize = int(config['denseSize'])\n",
    "    latentDim = int(config['latentDim'])\n",
    "    #vae:\n",
    "    alpha = 5\n",
    "    batchSize = int(config['batchSize'])\n",
    "    nEpochMax = int(config['nEpochData'])\n",
    "    nEpochTrain = int(config['nEpochTrain'])\n",
    "    learnRate = float(config['learningRate'])\n",
    "#     validationSplit = float(config['validationSplit'])\n",
    "\n",
    "    return (catPath, labPath, outputDir, sizeTestSet, sizeValSet, roiFile,\n",
    "            bands, sizeCutOut, nEpochMax, nEpochTrain, sizeStep, stride, file_DMGinfo, normThreshold,\n",
    "            filter1, filter2, kernelSize1,kernelSize2, denseSize, latentDim,\n",
    "            alpha, batchSize,learnRate)\n",
    "\n",
    "# parse input arguments\n",
    "# config = config if config is not None else \"train-vae.ini\"\n",
    "config = '/Users/tud500158/Library/Mobile Documents/com~apple~CloudDocs/Documents/Documents - TUD500158/github/AutomatedDamageDetection/scripts/train-vae/'\n",
    "config = os.path.join(config,'train-vae-S1.ini')\n",
    "catPath, labPath, outputDir, sizeTestSet, sizeValSet, roiFile, bands, \\\n",
    "    sizeCutOut, nEpochmax, nEpochTrain, sizeStep, stride, file_DMGinfo, normThreshold, \\\n",
    "    filter1, filter2, kernelSize1, kernelSize2, denseSize, latentDim, \\\n",
    "    alpha, batchSize,learnRate = parse_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import os\n",
    "learnRate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(normThreshold)\n",
    "normThreshold = [5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [33]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m a,b\u001b[38;5;241m=\u001b[39mnormThreshold\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "a,b=normThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalised to -15.0-0.0\n"
     ]
    }
   ],
   "source": [
    "print('Normalised to {:.1f}-{:.1f}'.format(a, b) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
