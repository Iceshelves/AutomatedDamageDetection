{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import rioxarray as rioxr\n",
    "import geopandas as gpd\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "import VAE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'inputDirectory':'~/input',\n",
    " 'outputDirectory':'~/output',         \n",
    " 'bands':[1,2,3],\n",
    " 'sizeCutOut':20,\n",
    " 'nEpochMax':2,\n",
    " 'sizeStep':5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_list(inputDir):\n",
    "    inputList = [t for t in pathlib.Path(inputDir).glob('*.tif')]\n",
    "    return inputList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_config(config):\n",
    "    if pathlib.Path(config).is_file():\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    else :\n",
    "        inputDir = config['inputDirectory']\n",
    "        outputDir = config['outputDirectory']\n",
    "        bands = config['bands']\n",
    "        sizeCutOut = config['sizeCutOut']\n",
    "        nEpochMax = config['nEpochMax']\n",
    "        sizeStep = config['sizeStep']\n",
    "        \n",
    "    return inputDir, outputDir, bands, sizeCutOut, nEpochmax, sizeStep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define Dataset class and methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    \n",
    "    def __init__(self, tile_list, cutout_size, bands, offset=0, stride=None, num_tiles=None, shuffle_tiles=False):\n",
    "        self.cutout_size = cutout_size\n",
    "        self.bands = bands\n",
    "        self.stride = stride if stride is not None else self.cutout_size\n",
    "        _num_tiles = num_tiles if num_tiles is not None else len(tile_list)\n",
    "        self.tiles = random.sample(tile_list, _num_tiles) if shuffle_tiles else tile_list[:_num_tiles]\n",
    "        if offset >= cutout_size:\n",
    "            raise ValueError(\n",
    "                \"offset larger than window size - set \"\n",
    "                \"offset to {}\".format(sizeCutOut%offset)\n",
    "            )\n",
    "        self.offset = offset\n",
    "        \n",
    "        self.mask = None\n",
    "        self.buffer = None\n",
    "        self.invert = None\n",
    "        self.all_touched = None\n",
    "    \n",
    "    def set_mask(self, geometry, crs, buffer=None, invert=False, all_touched=False):\n",
    "        \"\"\" Mask a selection of the pixels using a geometry.\"\"\"\n",
    "        self.mask = gpd.GeoSeries({\"geometry\": geometry}, crs=crs)\n",
    "        self.buffer = buffer if buffer is not None else 0\n",
    "        self.invert = invert\n",
    "        self.all_touched = all_touched\n",
    "    \n",
    "    def to_tf(self):\n",
    "        \"\"\" Obtain dataset as a tensorflow `Dataset` object. \"\"\"\n",
    "        ds = tf.data.Dataset.from_generator(\n",
    "            self._generate_cutouts,  \n",
    "            output_types=(tf.float64, tf.float64, tf.float32), \n",
    "            output_shapes=(\n",
    "                None,  # x\n",
    "                None,  # y\n",
    "                (None, None, self.cutout_size, self.cutout_size)  # samples, bands, x_win, y_win\n",
    "            )\n",
    "        )\n",
    "        return ds.flat_map(lambda x,y,z: tf.data.Dataset.from_tensor_slices((x,y,z)))\n",
    "\n",
    "    def _generate_cutouts(self):\n",
    "        \"\"\" \n",
    "        Iterate over (a selection of) the tiles yielding all \n",
    "        cutouts for each of them.\n",
    "        \"\"\"\n",
    "        for tile in self.tiles:\n",
    "            \n",
    "            print(f\"Reading tile {tile}!\")\n",
    "            \n",
    "            # read tile\n",
    "            da = rioxr.open_rasterio(tile).astype(\"float32\")  # needed to mask with NaN's\n",
    "            \n",
    "            #select bands\n",
    "            if self.bands is not None:\n",
    "                if type(self.bands) is not list:\n",
    "                    da = da.sel(band=[self.bands])\n",
    "                else:\n",
    "                    da = da.sel(band=self.bands)\n",
    "                    \n",
    "                \n",
    "            \n",
    "            if self.mask is not None:\n",
    "                mask = self.mask.to_crs(da.spatial_ref.crs_wkt)\n",
    "                geometry = mask.unary_union.buffer(self.buffer)\n",
    "                da = da.rio.clip([geometry], drop=True, invert=self.invert, all_touched=self.all_touched)\n",
    "            \n",
    "            # apply offset\n",
    "            da = da.shift(x=self.offset, y=self.offset)  # only shift data, not coords\n",
    "            da['x'] = da.x.shift(x=self.offset)\n",
    "            da['y'] = da.y.shift(y=self.offset)\n",
    "\n",
    "            # generate windows\n",
    "            da = da.rolling(x=self.cutout_size, y=self.cutout_size)\n",
    "            da = da.construct({'x': 'x_win', 'y': 'y_win'}, stride=self.stride)\n",
    "\n",
    "            # drop NaN-containing windows\n",
    "            da = da.stack(sample=('x', 'y'))\n",
    "            da = da.dropna(dim='sample', how='any')\n",
    "            yield (da.sample.coords['x'], \n",
    "                   da.sample.coords['y'], \n",
    "                   da.data.transpose(3, 1, 2, 0))  # samples, x_win, y_win, bands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Begin feedloop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDir, outputDir, _, sizeCutOut, nEpochmax, sizeStep = read_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputList = create_input_list(inputDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochCounter = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_file(\"ne_10m_antarctic_ice_shelves_polys/ne_10m_antarctic_ice_shelves_polys.shp\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# using datetime module for naming the current model, so that old models do not get overwritten\n",
    "import datetime;   \n",
    "# ct stores current time \n",
    "ct = datetime.datetime.now() \n",
    "# ts store timestamp of current time \n",
    "ts = ct.timestamp() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#make vae \n",
    "\n",
    "encoder_inputs, encoder, z , z_mean, z_log_var = VAE.make_encoder()\n",
    "decoder  = VAE.make_decoder()\n",
    "vae = VAE.make_vae(encoder_inputs, z, decoder)\n",
    "vae.compile(optimizer=keras.optimizers.Adam())\n",
    "path = os.path.join(outputDir, '/model_' + str(int(ts))) \n",
    "vae.save(path + '_epoch_' + str(epochCounter -1) )# change it: make a call to os to create a path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_data # should NOT be balanced\n",
    "#validation_data = .....# should be balanced \n",
    "\n",
    "while epochcounter < nEpochsMax:\n",
    "    offset = epochcounter*sizeStep\n",
    "    #randomly permutate inputList for each epoch\n",
    "    #inputList = list(np.random.permutation(inputList))\n",
    "    #construct Dataset\n",
    "    inData = Dataset(inputList,sizeCutOut,offset=offset,shuffle_tiles=True)\n",
    "    inData.set_mask(gdf.unary_union, crs=gdf.crs)\n",
    "    dataSet = inData.to_tf()\n",
    "    \n",
    "    \"\"\"\n",
    "    this buffersize sould correpond to 6 full tiles, \n",
    "    so 7 in total in memory\n",
    "    \"\"\" \n",
    "    dataSet.shuffle(buffersize=3000000).batch(64,drop_remainder=True)\n",
    "    \n",
    "    #normalization\n",
    "    data = dataSet # should be split in train and test\n",
    "    imax = np.max(data) # 15.000-ish\n",
    "    data = (data+0.1)/ (imax+1)\n",
    "    \"\"\"\n",
    "    network to be addded below\n",
    "    \"\"\"\n",
    "    vae =  keras.models.load_model(path + '_epoch_' + str(epochCounter -1))# change it: make a call to os to create a path\n",
    "    \n",
    "    vae.fit(training_data, epochs=1, validation_data = validation_data)\n",
    "    vae.save(path + '_epoch_' + str(epochCounter))   # change it: make a call to os to create a path\n",
    "         \n",
    "             \n",
    "    #repeat from here        \n",
    "    epochcounter = epochcounter + 1\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # embedding with tSNE here (for all models?) or, if you want to choose the model, in another script"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
